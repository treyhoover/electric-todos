---
description: electricSQL, Electric SQL, Electric
alwaysApply: false
---
```
# TypeScript client
The TypeScript client is a higher-level client interface that wraps the [HTTP API](/docs/api/http) to make it easy to sync [Shapes](/docs/guides/shapes) in the web browser and other JavaScript environments.
Defined in [packages/typescript-client](https://github.com/electric-sql/electric/tree/main/packages/typescript-client), it provides a [ShapeStream](#shapestream) primitive to subscribe to a change stream and a [Shape](#shape) primitive to get the whole shape whenever it changes.
## Install
The client is published on NPM as [`@electric-sql/client`](https://www.npmjs.com/package/@electric-sql/client):
```sh
npm i @electric-sql/client
```
## How to use
The client exports:
- a [`ShapeStream`](#shapestream) class for consuming a [Shape Log](../http#shape-log); and
- a [`Shape`](#shape) class for materialising the log stream into a shape object
These compose together, e.g.:
```ts
import { ShapeStream, Shape } from '@electric-sql/client'
const stream = new ShapeStream({
  url: `http://localhost:3000/v1/shape`,
  params: {
    table: 'items'
  }
})
const shape = new Shape(stream)
// The callback runs every time the Shape data changes.
shape.subscribe(data => console.log(data))
```
### ShapeStream
The [`ShapeStream`](https://github.com/electric-sql/electric/blob/main/packages/typescript-client/src/client.ts#L163) is a low-level primitive for consuming a [Shape Log](../http#shape-log).
Construct with a shape definition and options and then either subscribe to the shape log messages directly or pass into a [`Shape`](#shape) to materialise the stream into an object.
```tsx
import { ShapeStream } from '@electric-sql/client'
// Passes subscribers rows as they're inserted, updated, or deleted
const stream = new ShapeStream({
  url: `http://localhost:3000/v1/shape`,
  params: {
    table: `foo`
  }
})
stream.subscribe(messages => {
  // messages is an array with one or more row updates
  // and the stream will wait for all subscribers to process them
  // before proceeding
})
```
#### Options
The `ShapeStream` constructor takes [the following options](https://github.com/electric-sql/electric/blob/main/packages/typescript-client/src/client.ts#L39):
```ts
/**
 * Options for constructing a ShapeStream.
 */
export interface ShapeStreamOptions<T = never> {
  /**
   * The full URL to where the Shape is hosted. This can either be the Electric
   * server directly or a proxy. E.g. for a local Electric instance, you might
   * set `http://localhost:3000/v1/shape`
   */
  url: string
  /**
   * PostgreSQL-specific parameters for the shape.
   * This includes table, where clause, columns, and replica settings.
   */
  params: {
    /**
     * The root table for the shape.
     */
    table: string
    /**
     * The where clauses for the shape.
     */
    where?: string
    /**
     * Positional where clause paramater values. These will be passed to the server
     * and will substitute `$i` parameters in the where clause.
     *
     * It can be an array (positional arguments start at 1, the array will be mapped
     * accordingly), or an object with keys matching the used positional parameters in the where clause.
     *
     * If where clause is `id = $1 or id = $2`, params must have keys `"1"` and `"2"`, or be an array with length 2.
     */
    params?: Record<`${number}`, string> | string[]
    /**
     * The columns to include in the shape.
     * Must include primary keys, and can only include valid columns.
     */
    columns?: string[]
    /**
     * If `replica` is `default` (the default) then Electric will only send the
     * changed columns in an update.
     *
     * If it's `full` Electric will send the entire row with both changed and
     * unchanged values. `old_value` will also be present on update messages,
     * containing the previous value for changed columns.
     *
     * Setting `replica` to `full` will obviously result in higher bandwidth
     * usage and so is not recommended.
     */
    replica?: Replica
    /**
     * Additional request parameters to attach to the URL.
     * These will be merged with Electric's standard parameters.
     */
    [key: string]: string | string[] | undefined
  }
  /**
   * The "offset" on the shape log. This is typically not set as the ShapeStream
   * will handle this automatically. A common scenario where you might pass an offset
   * is if you're maintaining a local cache of the log. If you've gone offline
   * and are re-starting a ShapeStream to catch-up to the latest state of the Shape,
   * you'd pass in the last offset and shapeHandle you'd seen from the Electric server
   * so it knows at what point in the shape to catch you up from.
   */
  offset?: Offset
  /**
   * Similar to `offset`, this isn't typically used unless you're maintaining
   * a cache of the shape log.
   */
  shapeHandle?: string
  /**
   * HTTP headers to attach to requests made by the client.
   * Can be used for adding authentication headers.
   */
  headers?: Record<string, string>
  /**
   * Automatically fetch updates to the Shape. If you just want to sync the current
   * shape and stop, pass false.
   */
  subscribe?: boolean
  /**
   * Signal to abort the stream.
   */
  signal?: AbortSignal
  /**
   * Custom fetch client implementation.
   */
  fetchClient?: typeof fetch
  /**
   * Custom parser for handling specific Postgres data types.
   */
  parser?: Parser<T>
  /**
   * A function for handling errors.
   * This is optional, when it is not provided any shapestream errors will be thrown.
   * If the function returns an object containing parameters and/or headers
   * the shapestream will apply those changes and try syncing again.
   * If the function returns void the shapestream is stopped.
   */
  onError?: ShapeStreamErrorHandler
  backoffOptions?: BackoffOptions
}
type RetryOpts = {
  params?: ParamsRecord
  headers?: Record<string, string>
}
type ShapeStreamErrorHandler = (
  error: Error
) => void | RetryOpts | Promise<void | RetryOpts>
```
certain parameter names are reserved for Electric's internal use and cannot be used in custom params:
- `offset`
- `handle`
- `live`
- `cursor`
- `source_id`
The following PostgreSQL-specific parameters should be included within the `params` object:
- `table` - The root table for the shape
- `where` - SQL where clause for filtering rows
- `params` - Values for positional parameters in the where clause (e.g. `$1`)
- `columns` - List of columns to include
- `replica` - Controls whether to send full or partial row updates
Example with PostgreSQL-specific parameters:
```typescript
const stream = new ShapeStream({
  url: 'http://localhost:3000/v1/shape',
  params: {
    table: 'users',
    where: 'age > $1',
    columns: ['id', 'name', 'email'],
    params: ["18"],
    replica: 'full'
  }
})
```
You can also include additional custom parameters in the `params` object alongside the PostgreSQL-specific ones:
```typescript
const stream = new ShapeStream({
  url: 'http://localhost:3000/v1/shape',
  params: {
    table: 'users',
    customParam: 'value'
  }
})
```
#### Dynamic Options
Both `params` and `headers` support function options that are resolved when needed. These functions can be synchronous or asynchronous:
```typescript
const stream = new ShapeStream({
  url: 'http://localhost:3000/v1/shape',
  params: {
    table: 'items',
    userId: () => getCurrentUserId(),
    filter: async () => await getUserPreferences()
  },
  headers: {
    'Authorization': async () => `Bearer ${await getAccessToken()}`,
    'X-Tenant-Id': () => getCurrentTenant()
  }
})
```
Function options are resolved in parallel, making this pattern efficient for multiple async operations like fetching auth tokens and user context. Common use cases include:
- Authentication tokens that need to be refreshed
- User-specific parameters that may change
- Dynamic filtering based on current state
- Multi-tenant applications where context determines the request
#### Messages
A `ShapeStream` consumes and emits a stream of messages. These messages can either be a `ChangeMessage` representing a change to the shape data:
```ts
export type ChangeMessage<T extends Row<unknown> = Row> = {
  key: string
  value: T
  old_value?: Partial<T> // Only provided for updates if `replica` is `full`
  headers: Header & { operation: `insert` | `update` | `delete` }
  offset: Offset
}
```
Or a `ControlMessage`, representing an instruction to the client, as [documented here](../http#control-messages).
#### Parsing and Custom Parsing
To understand the type of each column in your shape, you can check the `electric-schema` response header in the shape response. This header contains the PostgreSQL type information for each column.
By default, when constructing a `ChangeMessage.value`, `ShapeStream` parses the following Postgres types into native JavaScript values:
- `int2`, `int4`, `float4`, and `float8` are parsed into JavaScript `Number`
- `int8` is parsed into a JavaScript `BigInt`
- `bool` is parsed into a JavaScript `Boolean`
- `json` and `jsonb` are parsed into JavaScript values/arrays/objects using `JSON.parse`
- Postgres Arrays are parsed into JavaScript arrays, e.g. "{{1,2},{3,4}}" is parsed into `[[1,2],[3,4]]`
All other types aren't parsed and are left in the string format as they were served by the HTTP endpoint.
You can extend the default parsing behavior by defining custom parsers for specific PostgreSQL data types. This is particularly useful when you want to transform string representations of dates, JSON, or other complex types into their corresponding JavaScript objects. Here's an example:
```ts
// Define row type
type CustomRow = {
  id: number
  title: string
  created_at: Date  // We want this to be a Date object
}
const stream = new ShapeStream<CustomRow>({
  url: 'http://localhost:3000/v1/shape',
  params: {
    table: 'posts'
  },
  parser: {
    // Parse timestamp columns into JavaScript Date objects
    timestamptz: (date: string) => new Date(date)
  }
})
const shape = new Shape(stream)
shape.subscribe(data => {
  console.log(data.created_at instanceof Date) // true
})
```
#### Replica full
By default Electric sends the modified columns in an update message, not the complete row. - an `insert` operation contains the full row
- an `update` operation contains the primary key column(s) and the changed columns
- a `delete` operation contains just the primary key column(s)
If you'd like to receive the full row value for updates and deletes, you can set the `replica` option of your `ShapeStream` to `full`:
```tsx
import { ShapeStream } from "@electric-sql/client"
const stream = new ShapeStream({
  url: `http://localhost:3000/v1/shape`,
  params: {
    table: `foo`,
    replica: `full`
  }
})
```
When using `replica=full`, the returned rows will include:
- on `insert` the new value in `msg.value`
- on `update` the new value in `msg.value` and the previous value in `msg.old_value` for any changed columns - the full previous state can be reconstructed by combining the two
- on `delete` the full previous value in `msg.value`
This is less efficient and will use more bandwidth for the same shape (especially for tables with large static column values). Note also that shapes with different `replica` settings are distinct, even for the same table and where clause combination.
#### Authentication with Dynamic Tokens
When working with authentication tokens that need to be refreshed, the recommended approach is to use a function-based header:
```ts
const stream = new ShapeStream({
  url: 'http://localhost:3000/v1/shape',
  params: {
    table: 'items'
  },
  headers: {
    'Authorization': async () => `Bearer ${await getToken()}`
  },
  onError: async (error) => {
    if (error instanceof FetchError && error.status === 401) {
      // Force token refresh
      await refreshToken()
      // Return empty object to trigger a retry with the new token
      // that will be fetched by our function-based header
      return {}
    }
    // Rethrow errors we can't handle
    throw error
  }
})
```
This approach automatically handles token refresh as the function is called each time a request is made. You can also combine this with an error handler for more complex scenarios.
### Shape
The [`Shape`](https://github.com/electric-sql/electric/blob/main/packages/typescript-client/src/shape.ts) is the main primitive for working with synced data.
It takes a [`ShapeStream`](#shapestream), consumes the stream, materialises it into a Shape object and notifies you when this changes.
```tsx
import { ShapeStream, Shape } from '@electric-sql/client'
const stream = new ShapeStream({
  url: `http://localhost:3000/v1/shape`,
  params: {
    table: `foo`
  }
})
const shape = new Shape(stream)
// Returns promise that resolves with the latest shape data once it's fully loaded
await shape.rows
// passes subscribers shape data when the shape updates
shape.subscribe(({ rows }) => {
  // rows is an array of the latest value of each row in a shape.
})
```
### Subscribing to updates
The `subscribe` method allows you to receive updates whenever the shape changes. It takes two arguments:
1. A message handler callback (required)
2. An error handler callback (optional)
```typescript
const stream = new ShapeStream({
  url: 'http://localhost:3000/v1/shape',
  params: {
    table: 'issues'
  }
})
// Subscribe to both message and error handlers
stream.subscribe(
  (messages) => {
    // Process messages
    console.log('Received messages:', messages)
  },
  (error) => {
    // Get notified about errors
    console.error('Error in subscription:', error)
  }
)
```
You can have multiple active subscriptions to the same stream. Each subscription will receive the same messages, and the stream will wait for all subscribers to process their messages before proceeding.
To stop receiving updates, you can either:
- Unsubscribe a specific subscription using the function returned by `subscribe`
- Unsubscribe all subscriptions using `unsubscribeAll()`
```typescript
// Store the unsubscribe function
const unsubscribe = stream.subscribe(messages => {
  console.log('Received messages:', messages)
})
// Later, unsubscribe this specific subscription
unsubscribe()
// Or unsubscribe all subscriptions
stream.unsubscribeAll()
```
### Error Handling
The ShapeStream provides two ways to handle errors:
1. Using the `onError` handler (recommended):
```typescript
const stream = new ShapeStream({
  url: 'http://localhost:3000/v1/shape',
  params: {
    table: 'issues'
  },
  onError: (error) => {
    // Handle all stream errors here
    if (error instanceof FetchError) {
      console.error('HTTP error:', error.status, error.message)
    } else {
      console.error('Stream error:', error)
    }
  }
})
```
If no `onError` handler is provided, the ShapeStream will throw errors that occur during streaming.
2. Individual subscribers can optionally handle errors specific to their subscription:
```typescript
stream.subscribe(
  (messages) => {
    // Process messages
  },
  (error) => {
    // Handle errors for this specific subscription
    console.error('Subscription error:', error)
  }
)
```
#### Error Types
The following error types may be encountered:
**Initialization Errors** (thrown by constructor):
- `MissingShapeUrlError`: Missing required URL parameter
- `InvalidSignalError`: Invalid AbortSignal instance
- `ReservedParamError`: Using reserved parameter names
**Runtime Errors** (handled by `onError` or thrown):
- `FetchError`: HTTP errors during shape fetching
- `FetchBackoffAbortError`: Fetch aborted using AbortSignal
- `MissingShapeHandleError`: Missing required shape handle
- `ParserNullValueError`: Parser encountered NULL value in a column that doesn't allow NULL values
See the [Demos](/demos) and [integrations](/docs/integrations/react) for more usage examples.
# React
React is a popular library for building declarative, component-based UI.
## Electric and React
Electric has first-class support for React. We maintain a [react-hooks](https://github.com/electric-sql/electric/tree/main/packages/react-hooks) package that provides a number of [React Hooks](https://react.dev/reference/react/hooks) to bind Shape data to your components.
## How to use
### Install
The package is published on NPM as [`@electric-sql/react`](https://www.npmjs.com/package/@electric-sql/react). Install using e.g.:
```shell
npm i @electric-sql/react
```
### `useShape`
[`useShape`](https://github.com/electric-sql/electric/blob/main/packages/react-hooks/src/react-hooks.tsx#L131) binds a materialised [Shape](/docs/api/clients/typescript#shape) to a state variable. ```tsx
import { useShape } from '@electric-sql/react'
const MyComponent = () => {
  const { isLoading, data } = useShape<{title: string}>({
    url: `http://localhost:3000/v1/shape`,
    params: {
      table: 'items'
    }
  })
  if (isLoading) {
    return <div>Loading ...</div>
  }
  return (
    <div>
      {data.map(item => <div>{item.title}</div>)}
    </div>
  )
}
```
You can also include additional PostgreSQL-specific parameters:
```tsx
const MyFilteredComponent = () => {
  const { isLoading, data } = useShape<{id: number, title: string}>({
    url: `http://localhost:3000/v1/shape`,
    params: {
      table: 'items',
      where: 'status = \'active\'',
      columns: ['id', 'title']
    }
  })
  // ...
}
```
`useShape` takes the same options as [ShapeStream](/docs/api/clients/typescript#options). The return value is a `UseShapeResult`:
```tsx
export interface UseShapeResult<T extends Row<unknown> = Row> {
  /**
   * The array of rows that make up the materialised Shape.
   * @type {T[]}
   */
  data: T[]
  /**
   * The Shape instance used by this useShape
   * @type {Shape<T>}
   */
  shape: Shape<T>
  /** True during initial fetch. False afterwise. */
  isLoading: boolean
  /** Unix time at which we last synced. Undefined when `isLoading` is true. */
  lastSyncedAt?: number
  /** Unix time at which we last synced. Undefined when `isLoading` is true. */
  isError: boolean
  error: Shape<T>[`error`]
}
```
### `preloadShape`
[`preloadShape`](https://github.com/electric-sql/electric/blob/main/packages/react-hooks/src/react-hooks.tsx#L17) is useful to call in route loading functions or elsewhere when you want to ensure Shape data is loaded before rendering a route or component.
```tsx
export const clientLoader = async () => {
  return await preloadShape({
    url: `http://localhost:3000/v1/shape`,
    params: {
      table: 'items'
    }
  })
}
```
You can also preload filtered data:
```tsx
export const filteredLoader = async () => {
  return await preloadShape({
    url: `http://localhost:3000/v1/shape`,
    params: {
      table: 'items',
      where: 'category = \'electronics\'',
      columns: ['id', 'name', 'price']
    }
  })
}
```
It takes the same options as [ShapeStream](/docs/api/clients/typescript#options).
### `getShapeStream`
[`getShapeStream`](https://github.com/electric-sql/electric/blob/main/packages/react-hooks/src/react-hooks.tsx#L30) get-or-creates a `ShapeStream` off the global cache.
```tsx
const itemsStream = getShapeStream<Item>({
  url: `http://localhost:3000/v1/shape`,
  params: {
    table: 'items'
  }
})
```
### `getShape`
[`getShape`](https://github.com/electric-sql/electric/blob/main/packages/react-hooks/src/react-hooks.tsx#L49) get-or-creates a `Shape` off the global cache.
```tsx
const itemsShape = getShape<Item>({
  url: `http://localhost:3000/v1/shape`,
  params: {
    table: 'items'
  }
})
```
### How to abort a shape subscription â€” `AbortController`
If you'd like to abort the shape's subscription to live updates e.g. after unmounting a component or navigating away from a route, you can use the [`AbortController`](https://developer.mozilla.org/en-US/docs/Web/API/AbortController).
The following is a simple example which aborts the subscription when the component is unmounted.
```tsx
function MyComponent() {
  const [controller, _] = useState(new AbortController())
  const { data } = useShape({
    ...
    signal: controller.signal
  })
  useEffect(() => {
    return () {
      // Live updates are now disabled.
      controller.abort()
    }
  }, [])
  ...
}
```
if you have multiple components using the same component, this will stop updates for all subscribers. Which is probably not what you want. We plan to add a better API for unsubscribing from updates & cleaning up shapes that are no longer needed. If interested, please file an issue to start a discussion.
# Shapes
Shapes are the core primitive for controlling sync in the ElectricSQL system.
## What is a Shape?
Electric syncs little subsets of your Postgres data into local apps and services. Those subsets are defined using Shapes.
### Little subsets
Imagine a Postgres database in the cloud with lots of data stored in it. It's often impractical or unwanted to sync all of this data over the network onto a local device.
A shape is a way of defining a subset of that data that you'd like to sync into a local app. Defining shapes allows you to sync just the data you want and just the data that's practical to sync onto the local device.
A client can choose to sync one shape, or lots of shapes. Many clients can sync the same shape. Multiple shapes can overlap.
## Defining shapes
Shapes are defined by:
- a [table](#table), such as `items`
- an optional [where clause](#where-clause) to filter which rows are included in the shape
- an optional [columns](#columns) clause to select which columns are included
A shape contains all of the rows in the table that match the where clause, if provided. If a columns clause is provided, the synced rows will only contain those selected columns.
[WARNING] > Shapes are currently [single table](#single-table). Shape definitions are [immutable](#immutable).
### Table
This is the root table of the shape. All shapes must specify a table and it must match a table in your Postgres database.
The value can be just a tablename like `projects`, or can be a qualified tablename prefixed by the database schema using a `.` delimiter, such as `foo.projects`. If you don't provide a schema prefix, then the table is assumed to be in the `public.` schema.
#### Partitioned Tables
Electric supports subscribing to [declaratively partitioned tables](https://www.postgresql.org/docs/current/ddl-partitioning.html#DDL-PARTITIONING-DECLARATIVE), both individual partitions and the root table of all partitions.
Consider the following partitioned schema:
```sql
CREATE TABLE measurement (
    city_id         int not null,
    logdate         date not null,
    peaktemp        int,
    unitsales       int
) PARTITION BY RANGE (logdate);
CREATE TABLE measurement_y2025m02 PARTITION OF measurement
    FOR VALUES FROM ('2025-02-01') TO ('2025-03-01');
CREATE TABLE measurement_y2025m03 PARTITION OF measurement
    FOR VALUES FROM ('2025-03-01') TO ('2025-04-01');
```
We create 2 shapes, one on the root table `measurement` and one on the `measurement_y2025m03` partition:
```sh
curl -i 'http://localhost:3000/v1/shape?table=measurement&offset=-1'
curl -i 'http://localhost:3000/v1/shape?table=measurement_y2025m03&offset=-1'
```
The shape based on the `measurement_y2025m03` partition will only receive writes that fall within the partition range, that is with `logdate >= '2025-02-01' AND  logdate < '2025-03-01'` whereas the shape based on the root `measurements` table will receive all writes to all partitions.
### Where clause
Shapes can define an optional where clause to filter out which rows from the table are included in the shape. Only rows that match the where clause will be included.
The where clause must be a valid [PostgreSQL query expression](https://www.postgresql.org/docs/current/queries-table-expressions.html#QUERIES-WHERE) in SQL syntax, e.g.:
- `title='Electric'`
- `status IN ('backlog', 'todo')`
Where clauses support:
1. columns of numerical types, `boolean`, `uuid`, `text`, `interval`, date and time types (with the exception of `timetz`), [Arrays](https://github.com/electric-sql/electric/issues/1767) (but not yet [Enums](https://github.com/electric-sql/electric/issues/1709), except when explicitly casting them to `text`)
2. operators that work on those types: arithmetics, comparisons, logical/boolean operators like `OR`, string operators like `LIKE`, etc.
3. positional placeholders, like `$1`, values for which must be provided alongside the where clause.
You can use `AND` and `OR` to group multiple conditions, e.g.:
- `title='Electric' OR title='SQL'`
- `title='Electric' AND status='todo'`
Where clauses are limited in that they:
1. can only refer to columns in the target row
1. can't perform joins or refer to other tables
1. can't use non-deterministic SQL functions like `count()` or `now()`
When constructing a where clause with user input as a filter, it's recommended to use a positional placeholder (`$1`) to avoid
SQL injection-like situations. if filtering a table on a user id, it's better to use `where=user = $1` with
`params[1]=provided_id`. If not using positional placeholders and constructing where clauses yourself, take care to SQL-escape user input.
See [`known_functions.ex`](https://github.com/electric-sql/electric/blob/main/packages/sync-service/lib/electric/replication/eval/env/known_functions.ex) and [`parser.ex`](https://github.com/electric-sql/electric/blob/main/packages/sync-service/lib/electric/replication/eval/parser.ex) for the source of truth on which types, operators and functions are currently supported. If you need a feature that isn't supported yet, please [raise a feature request](https://github.com/electric-sql/electric/discussions/categories/feature-requests).
[WARNING] > Where clause evaluation impacts [data throughput](#throughput). Some where clauses are [optimized](#optimized-where-clauses).
### Columns
This is an optional list of columns to select. When specified, only the columns listed are synced. When not specified all columns are synced.
- `columns=id,title,status` - only include the `id`, `title` and `status` columns
- `columns=id,"Status-Check"` - only include `id` and `Status-Check` columns, quoting the identifiers where necessary
The specified columns must always include the primary key column(s), and should be formed as a comma separated list of column names - exactly as they are in the database schema. If the identifier was defined as case sensitive and/or with special characters, then you must quote it.
## Subscribing to shapes
Local clients establish shape subscriptions, typically using [client libraries](/docs/api/clients/typescript). These sync data from the [Electric sync engine](/product/electric) into the client using the [HTTP API](/docs/api/http).
The sync service maintains shape subscriptions and streams any new data and data changes to the local client. In the client, shapes can be held as objects in memory, using a [`useShape`](/docs/integrations/react) hook, or in a normalised store or database like [PGlite](/product/pglite).
### HTTP
You can sync shapes manually using the
GET /v1/shape endpoint. First make an initial sync request to get the current data for the Shape, such as:
```sh
curl -i 'http://localhost:3000/v1/shape?table=foo&offset=-1'
```
Then switch into a live mode to use long-polling to receive real-time updates:
```sh
curl -i 'http://localhost:3000/v1/shape?table=foo&live=true&offset=...&handle=...'
```
These requests both return an array of [Shape Log](/docs/api/http#shape-log) entries. You can process these manually, or use a higher-level client.
### Typescript
You can use the [Typescript Client](/docs/api/clients/typescript) to process the Shape Log and materialised it into a `Shape` object for you.
First install using:
```sh
npm i @electric-sql/client
```
Instantiate a `ShapeStream` and materialise into a `Shape`:
```ts
import { ShapeStream, Shape } from '@electric-sql/client'
const stream = new ShapeStream({
  url: `http://localhost:3000/v1/shape`,
  params: {
    table: `foo`
  }
})
const shape = new Shape(stream)
// Returns promise that resolves with the latest shape data once it's fully loaded
await shape.rows
```
You can register a callback to be notified whenever the shape data changes:
```ts
shape.subscribe(({ rows }) => {
  // rows is an array of the latest value of each row in a shape.
})
```
Or you can use framework integrations like the [`useShape`](/docs/integrations/react) hook to automatically bind materialised shapes to your components.
See the [Quickstart](/docs/quickstart) and [HTTP API](/docs/api/http) docs for more information.
# Auth
How to do authentication and authorization with Electric. Including examples for [proxy](#proxy-auth) and [gatekeeper](#gatekeeper-auth) auth.
How to do auth with Electric.
Including examples for [proxy](#proxy-auth) and [gatekeeper](#gatekeeper-auth) auth.
## It's all HTTP
The golden rule with Electric is that it's [all just HTTP](/docs/api/http).
So when it comes to auth, you can use existing primitives, such as your API, middleware and external authorization services.
### Shapes are resources
With Electric, you sync data using [Shapes](/docs/guides/shapes) and shapes are just resources.
You access them by making a request to `GET /v1/shape`, with the [shape definition](/docs/guides/shapes#defining-shapes) in the query string (`?table=items`, etc.). You can authorise access to them exactly the same way you would any other web resource.
### Requests can be proxied
When you make a request to Electric, you can route it through an HTTP proxy or middleware stack.
You can proxy the request in your cloud, or at the edge, [in-front of a CDN](#cdn-proxy). Your auth logic can query your database, or call an external service. It's all completely up-to-you.
### Rules are optional
You *don't* have to codify your auth logic into a database rule system.
There's no need to use database rules to [secure data access](/docs/guides/security) when your sync engine runs over standard HTTP.
## Patterns
The two patterns we recommend and describe below, with code and examples, are:
- [proxy auth](#proxy-auth) - authorising Shape requests using a proxy
- [gatekeeper auth](#gatekeeper-auth) - using your API to generate shape-scoped access tokens
### Proxy auth
[WARNING] > See the [proxy-auth example](https://github.com/electric-sql/electric/tree/main/examples/proxy-auth) on GitHub for an example that implements this pattern.
The simplest pattern is to authorise Shape requests using a reverse-proxy.
The proxy can be your API, or a separate proxy service or edge-function. When you make a request to sync a shape, route it via your API/proxy, validate the user credentials and shape parameters, and then only proxy the data through if authorized.
1. add an `Authorization` header to your [`GET /v1/shape`](/docs/api/http#syncing-shapes) request
2. use the header to check that the client exists and has access to the shape
3. if not, return a `401` or `403` status to tell the client it doesn't have access
4. if the client does have access, proxy the request to Electric and stream the response back to the client
#### Example
When using the [Typescript client](/docs/api/clients/typescript), you can pass in a [`headers` option](/docs/api/clients/typescript#options) to add an `Authorization` header.
```tsx
const usersShape = (): ShapeStreamOptions => {
  const user = loadCurrentUser()
  return {
    url: new URL(`/api/shapes/users`, window.location.origin).href,
    headers: {
      authorization: `Bearer ${user.token}`
    }
  }
}
export default function ExampleComponent () {
  const { data: users } = useShape(usersShape())
}
```
Then for the `/api/shapes/users` route:
```tsx
export async function GET(
  request: Request,
) {
  const url = new URL(request.url)
  // Construct the upstream URL
  const originUrl = new URL(`http://localhost:3000/v1/shape`)
  // Copy over the relevant query params that the Electric client adds
  // so that we return the right part of the Shape log.
  url.searchParams.forEach((value, key) => {
    if ([`live`, `table`, `handle`, `offset`, `cursor`].includes(key)) {
      originUrl.searchParams.set(key, value)
    }
  })
  //
  // Authentication and authorization
  //
  const user = await loadUser(request.headers.get(`authorization`))
  // If the user isn't set, return 401
  if (!user) {
    return new Response(`user not found`, { status: 401 })
  }
  // Only query data the user has access to unless they're an admin.
  if (!user.roles.includes(`admin`)) {
    originUrl.searchParams.set(`where`, `"org_id" = ${user.org_id}`)
  }
  const response = await fetch(originUrl)
  // Fetch decompresses the body but doesn't remove the
  // content-encoding & content-length headers which would
  // break decoding in the browser.
  //
  // See https://github.com/whatwg/fetch/issues/1729
  const headers = new Headers(response.headers)
  headers.delete(`content-encoding`)
  headers.delete(`content-length`)
  return new Response(response.body, {
    status: response.status,
    statusText: response.statusText,
    headers,
  })
}
```
# Writes
How to do local writes and write-path sync with Electric.
Includes patterns for [online writes](#online-writes), [optimistic state](#optimistic-state), [shared persistent optimistic state](#shared-persistent) and [through-the-database sync](#through-the-db).
With accompanying code in the [write-patterns example](https://github.com/electric-sql/electric/tree/main/examples/write-patterns).
## Local writes with Electric
Electric does [read-path sync](/product/electric). It syncs data out-of Postgres, into local apps and services.
Electric does not do write-path sync. It doesn't provide (or prescribe) a built-in solution for getting data back into Postgres from local apps and services.
So how do you handle local writes with Electric?
Well, the [design philosophy](/blog/2024/07/17/electric-next) behind Electric is to be composable and [integrate with your existing stack](/blog/2024/11/21/local-first-with-your-existing-api). So, just as you can sync into [any client](/docs/guides/client-development) you like, you can implement writes in any way you like, using a variety of different patterns.
## Patterns
This guide describes four different patterns for handling writes with Electric. It shows code examples and discusses trade-offs to consider when choosing between them.
1. [online writes](#online-writes)
2. [optimistic state](#optimistic-state)
3. [shared persistent optimistic state](#shared-persistent)
4. [through-the-database sync](#through-the-db)
All of the patterns use Electric for the read-path sync (i.e.: to sync data from Postgres into the local app) and use a different approach for the write-path (i.e.: how they handle local writes and get data from the local app back into Postgres).
They are introduced in order of simplicity. So the simplest and easiest to implement first and the more powerful but more complex patterns further down - where you may prefer to reach for a [framework](#tools) rather than implement yourself.
### 1. Online writes
```tsx
import React from 'react'
import { v4 as uuidv4 } from 'uuid'
import { useShape } from '@electric-sql/react'
import api from '../../shared/app/client'
import { ELECTRIC_URL, envParams } from '../../shared/app/config'
type Todo = {
  id: string
  title: string
  completed: boolean
  created_at: Date
}
export default function OnlineWrites() {
  // Use Electric's `useShape` hook to sync data from Postgres
  // into a React state variable.
  const { isLoading, data } = useShape<Todo>({
    url: `${ELECTRIC_URL}/v1/shape`,
    params: {
      table: 'todos',
      ...envParams,
    },
    parser: {
      timestamptz: (value: string) => new Date(value),
    },
  })
  const todos = data ? data.sort((a, b) => +a.created_at - +b.created_at) : []
  // Handle user input events by making requests to the backend
  // API to create, update and delete todos.
  async function createTodo(event: React.FormEvent) {
    event.preventDefault()
    const form = event.target as HTMLFormElement
    const formData = new FormData(form)
    const title = formData.get('todo') as string
    const path = '/todos'
    const data = {
      id: uuidv4(),
      title: title,
      created_at: new Date(),
    }
    await api.request(path, 'POST', data)
    form.reset()
  }
  async function updateTodo(todo: Todo) {
    const path = `/todos/${todo.id}`
    const data = {
      completed: !todo.completed,
    }
    await api.request(path, 'PUT', data)
  }
  async function deleteTodo(event: React.MouseEvent, todo: Todo) {
    event.preventDefault()
    const path = `/todos/${todo.id}`
    await api.request(path, 'DELETE')
  }
  if (isLoading) {
    return <div className="loading">Loading -</div>
  }
  // prettier-ignore
  return (
    <div id="online-writes" className="example">
      <h3>1. Online writes</h3>
      <ul>
        {todos.map((todo) => (
          <li key={todo.id}>
            <label>
              <input type="checkbox" checked={todo.completed}
                  onChange={() => updateTodo(todo)}
              />
              <span className={`title ${ todo.completed ? 'completed' : '' }`}>
                { todo.title }
              </span>
            </label>
            <a href="#delete" className="close"
                onClick={(event) => deleteTodo(event, todo)}>
              &#x2715;</a>
          </li>
        ))}
        {todos.length === 0 && (
          <li>All done </li>
        )}
      </ul>
      <form onSubmit={createTodo}>
        <input type="text" name="todo"
            placeholder="Type here -"
            required
        />
        <button type="submit">
          Add
        </button>
      </form>
    </div>
  )
}
```
### 2. Optimistic state
```tsx
import React, { useOptimistic, useTransition } from 'react'
import { v4 as uuidv4 } from 'uuid'
import { matchBy, matchStream } from '@electric-sql/experimental'
import { useShape } from '@electric-sql/react'
import api from '../../shared/app/client'
import { ELECTRIC_URL, envParams } from '../../shared/app/config'
type Todo = {
  id: string
  title: string
  completed: boolean
  created_at: Date
}
type PartialTodo = Partial<Todo> & {
  id: string
}
type Write = {
  operation: 'insert' | 'update' | 'delete'
  value: PartialTodo
}
export default function OptimisticState() {
  const [isPending, startTransition] = useTransition()
  // Use Electric's `useShape` hook to sync data from Postgres
  // into a React state variable.
  //
  // we also unpack the `stream` from the useShape
  // return value, so that we can monitor it below to detect
  // local writes syncing back from the server.
  const { isLoading, data, stream } = useShape<Todo>({
    url: `${ELECTRIC_URL}/v1/shape`,
    params: {
      table: 'todos',
      ...envParams,
    },
    parser: {
      timestamptz: (value: string) => new Date(value),
    },
  })
  const sorted = data ? data.sort((a, b) => +a.created_at - +b.created_at) : []
  // Use React's built in `useOptimistic` hook. This provides
  // a mechanism to apply local optimistic state whilst writes
  // are being sent-to and syncing-back-from the server.
  const [todos, addOptimisticState] = useOptimistic(
    sorted,
    (synced: Todo[], { operation, value }: Write) => {
      switch (operation) {
        case 'insert':
          return synced.some((todo) => todo.id === value.id)
            ? synced
            : [...synced, value as Todo]
        case 'update':
          return synced.map((todo) =>
            todo.id === value.id ? { ...todo, ...value } : todo
          )
        case 'delete':
          return synced.filter((todo) => todo.id !== value.id)
      }
    }
  )
  // These are the same event handler functions from the online
  // example, extended with `startTransition` -> `addOptimisticState`
  // to apply local optimistic state.
  //
  // the local state is applied:
  //
  // 1. whilst the HTTP request is being made to the API server; and
  // 2. until the write syncs back through the Electric shape stream
  //
  // This is slightly different from most optimistic state examples
  // because we wait for the sync as well as the api request.
  async function createTodo(event: React.FormEvent) {
    event.preventDefault()
    const form = event.target as HTMLFormElement
    const formData = new FormData(form)
    const title = formData.get('todo') as string
    const path = '/todos'
    const data = {
      id: uuidv4(),
      title: title,
      created_at: new Date(),
      completed: false,
    }
    startTransition(async () => {
      addOptimisticState({ operation: 'insert', value: data })
      const fetchPromise = api.request(path, 'POST', data)
      const syncPromise = matchStream(
        stream,
        ['insert'],
        matchBy('id', data.id)
      )
      await Promise.all([fetchPromise, syncPromise])
    })
    form.reset()
  }
  async function updateTodo(todo: Todo) {
    const { id, completed } = todo
    const path = `/todos/${id}`
    const data = {
      id,
      completed: !completed,
    }
    startTransition(async () => {
      addOptimisticState({ operation: 'update', value: data })
      const fetchPromise = api.request(path, 'PUT', data)
      const syncPromise = matchStream(stream, ['update'], matchBy('id', id))
      await Promise.all([fetchPromise, syncPromise])
    })
  }
  async function deleteTodo(event: React.MouseEvent, todo: Todo) {
    event.preventDefault()
    const { id } = todo
    const path = `/todos/${id}`
    startTransition(async () => {
      addOptimisticState({ operation: 'delete', value: { id } })
      const fetchPromise = api.request(path, 'DELETE')
      const syncPromise = matchStream(stream, ['delete'], matchBy('id', id))
      await Promise.all([fetchPromise, syncPromise])
    })
  }
  if (isLoading) {
    return <div className="loading">Loading -</div>
  }
  // The template below the heading is identical to the other patterns.
  // prettier-ignore
  return (
    <div id="optimistic-state" className="example">
      <h3>
        <span className="title">
          2. Optimistic state
        </span>
        <span className={isPending ? 'pending' : 'pending hidden'} />
      </h3>
      <ul>
        {todos.map((todo) => (
          <li key={todo.id}>
            <label>
              <input type="checkbox" checked={todo.completed}
                  onChange={() => updateTodo(todo)}
              />
              <span className={`title ${ todo.completed ? 'completed' : '' }`}>
                { todo.title }
              </span>
            </label>
            <a href="#delete" className="close"
                onClick={(event) => deleteTodo(event, todo)}>
              &#x2715;</a>
          </li>
        ))}
        {todos.length === 0 && (
          <li>All done </li>
        )}
      </ul>
      <form onSubmit={createTodo}>
        <input type="text" name="todo"
            placeholder="Type here -"
            required
        />
        <button type="submit">
          Add
        </button>
      </form>
    </div>
  )
}
```
### 3. Shared persistent optimistic state
```tsx
import React, { useTransition } from 'react'
import { v4 as uuidv4 } from 'uuid'
import { subscribe, useSnapshot } from 'valtio'
import { proxyMap } from 'valtio/utils'
import { type Operation, ShapeStream } from '@electric-sql/client'
import { matchBy, matchStream } from '@electric-sql/experimental'
import { useShape } from '@electric-sql/react'
import api from '../../shared/app/client'
import { ELECTRIC_URL, envParams } from '../../shared/app/config'
const KEY = 'electric-sql/examples/write-patterns/shared-persistent'
type Todo = {
  id: string
  title: string
  completed: boolean
  created_at: Date
}
type PartialTodo = Partial<Todo> & {
  id: string
}
type LocalWrite = {
  id: string
  operation: Operation
  value: PartialTodo
}
// Define a shared, persistent, reactive store for local optimistic state.
const optimisticState = proxyMap<string, LocalWrite>(
  JSON.parse(localStorage.getItem(KEY) || '[]')
)
subscribe(optimisticState, () => {
  localStorage.setItem(KEY, JSON.stringify([...optimisticState]))
})
/*
 * Add a local write to the optimistic state
 */
function addLocalWrite(operation: Operation, value: PartialTodo): LocalWrite {
  const id = uuidv4()
  const write: LocalWrite = {
    id,
    operation,
    value,
  }
  optimisticState.set(id, write)
  return write
}
/*
 * Subscribe to the shape `stream` until the local write syncs back through it.
 * At which point, delete the local write from the optimistic state.
 */
async function matchWrite(
  stream: ShapeStream<Todo>,
  write: LocalWrite
): Promise<void> {
  const { operation, value } = write
  const matchFn =
    operation === 'delete'
      ? matchBy('id', value.id)
      : matchBy('write_id', write.id)
  try {
    await matchStream(stream, [operation], matchFn)
  } catch (_err) {
    return
  }
  optimisticState.delete(write.id)
}
/*
 * Make an HTTP request to send the write to the API server.
 * If the request fails, delete the local write from the optimistic state.
 * If it succeeds, return the `txid` of the write from the response data.
 */
async function sendRequest(
  path: string,
  method: string,
  { id, value }: LocalWrite
): Promise<void> {
  const data = {
    ...value,
    write_id: id,
  }
  let response: Response | undefined
  try {
    response = await api.request(path, method, data)
  } catch (_err) {
    // ignore
  }
  if (response === undefined || !response.ok) {
    optimisticState.delete(id)
  }
}
export default function SharedPersistent() {
  const [isPending, startTransition] = useTransition()
  // Use Electric's `useShape` hook to sync data from Postgres.
  const { isLoading, data, stream } = useShape<Todo>({
    url: `${ELECTRIC_URL}/v1/shape`,
    params: {
      table: 'todos',
      ...envParams,
    },
    parser: {
      timestamptz: (value: string) => new Date(value),
    },
  })
  const sorted = data ? data.sort((a, b) => +a.created_at - +b.created_at) : []
  // Get the local optimistic state.
  const localWrites = useSnapshot<Map<string, LocalWrite>>(optimisticState)
  const computeOptimisticState = (
    synced: Todo[],
    writes: LocalWrite[]
  ): Todo[] => {
    return writes.reduce(
      (synced: Todo[], { operation, value }: LocalWrite): Todo[] => {
        switch (operation) {
          case 'insert':
            return [...synced, value as Todo]
          case 'update':
            return synced.map((todo) =>
              todo.id === value.id ? { ...todo, ...value } : todo
            )
          case 'delete':
            return synced.filter((todo) => todo.id !== value.id)
          default:
            return synced
        }
      },
      synced
    )
  }
  const todos = computeOptimisticState(sorted, [...localWrites.values()])
  // These are the same event handler functions from the previous optimistic
  // state pattern, adapted to add the state to the shared, persistent store.
  async function createTodo(event: React.FormEvent) {
    event.preventDefault()
    const form = event.target as HTMLFormElement
    const formData = new FormData(form)
    const title = formData.get('todo') as string
    const path = '/todos'
    const data = {
      id: uuidv4(),
      title: title,
      completed: false,
      created_at: new Date(),
    }
    startTransition(async () => {
      const write = addLocalWrite('insert', data)
      const fetchPromise = sendRequest(path, 'POST', write)
      const syncPromise = matchWrite(stream, write)
      await Promise.all([fetchPromise, syncPromise])
    })
    form.reset()
  }
  async function updateTodo(todo: Todo) {
    const { id, completed } = todo
    const path = `/todos/${id}`
    const data = {
      id,
      completed: !completed,
    }
    startTransition(async () => {
      const write = addLocalWrite('update', data)
      const fetchPromise = sendRequest(path, 'PUT', write)
      const syncPromise = matchWrite(stream, write)
      await Promise.all([fetchPromise, syncPromise])
    })
  }
  async function deleteTodo(event: React.MouseEvent, todo: Todo) {
    event.preventDefault()
    const { id } = todo
    const path = `/todos/${id}`
    const data = {
      id,
    }
    startTransition(async () => {
      const write = addLocalWrite('delete', data)
      const fetchPromise = sendRequest(path, 'DELETE', write)
      const syncPromise = matchWrite(stream, write)
      await Promise.all([fetchPromise, syncPromise])
    })
  }
  if (isLoading) {
    return <div className="loading">Loading -</div>
  }
  // The template below the heading is identical to the other patterns.
  // prettier-ignore
  return (
    <div id="optimistic-state" className="example">
      <h3>
        <span className="title">
          3. Shared persistent
        </span>
        <span className={isPending ? 'pending' : 'pending hidden'} />
      </h3>
      <ul>
        {todos.map((todo) => (
          <li key={todo.id}>
            <label>
              <input type="checkbox" checked={todo.completed}
                  onChange={() => updateTodo(todo)}
              />
              <span className={`title ${ todo.completed ? 'completed' : '' }`}>
                { todo.title }
              </span>
            </label>
            <a href="#delete" className="close"
                onClick={(event) => deleteTodo(event, todo)}>
              &#x2715;</a>
          </li>
        ))}
        {todos.length === 0 && (
          <li>All done </li>
        )}
      </ul>
      <form onSubmit={createTodo}>
        <input type="text" name="todo"
            placeholder="Type here -"
            required
        />
        <button type="submit">
          Add
        </button>
      </form>
    </div>
  )
}
```
### 4. Through the database sync
The application code in [`index.tsx`](https://github.com/electric-sql/electric/blog/main/examples/write-patterns/patterns/4-through-the-db/index.tsx) stays very simple. Most of the complexity is abstracted into the local database schema, defined in [`local-schema.sql`](https://github.com/electric-sql/electric/blog/main/examples/write-patterns/patterns/4-through-the-db/local-schema.sql). The write-path sync utility in [`sync.ts`](https://github.com/electric-sql/electric/blog/main/examples/write-patterns/patterns/4-through-the-db/local-schema.sql) handles sending data to the server.
```tsx
import React, { useEffect, useState } from 'react'
import { v4 as uuidv4 } from 'uuid'
import {
  PGliteProvider,
  useLiveQuery,
  usePGlite,
} from '@electric-sql/pglite-react'
import { type PGliteWithLive } from '@electric-sql/pglite/live'
import loadPGlite from './db'
import ChangeLogSynchronizer from './sync'
type Todo = {
  id: string
  title: string
  completed: boolean
  created_at: Date
}
/*
 * Setup the local PGlite database, with automatic change detection and syncing.
 *
 * See `./local-schema.sql` for the local database schema, including view
 * and trigger machinery.
 *
 * See `./sync.ts` for the write-path sync utility, which listens to changes
 * using pg_notify, as per https://pglite.dev/docs/api#listen
 */
export default function Wrapper() {
  const [db, setDb] = useState<PGliteWithLive>()
  useEffect(() => {
    let isMounted = true
    let writePathSync: ChangeLogSynchronizer
    async function init() {
      const pglite = await loadPGlite()
      if (!isMounted) {
        return
      }
      writePathSync = new ChangeLogSynchronizer(pglite)
      writePathSync.start()
      setDb(pglite)
    }
    init()
    return () => {
      isMounted = false
      if (writePathSync !== undefined) {
        writePathSync.stop()
      }
    }
  }, [])
  if (db === undefined) {
    return <div className="loading">Loading -</div>
  }
  return (
    <PGliteProvider db={db}>
      <ThroughTheDB />
    </PGliteProvider>
  )
}
function ThroughTheDB() {
  const db = usePGlite()
  const results = useLiveQuery<Todo>('SELECT * FROM todos ORDER BY created_at')
  async function createTodo(event: React.FormEvent) {
    event.preventDefault()
    const form = event.target as HTMLFormElement
    const formData = new FormData(form)
    const title = formData.get('todo') as string
    await db.sql`
      INSERT INTO todos (
        id,
        title,
        completed,
        created_at
      )
      VALUES (
        ${uuidv4()},
        ${title},
        ${false},
        ${new Date()}
      )
    `
    form.reset()
  }
  async function updateTodo(todo: Todo) {
    const { id, completed } = todo
    await db.sql`
      UPDATE todos
        SET completed = ${!completed}
        WHERE id = ${id}
    `
  }
  async function deleteTodo(event: React.MouseEvent, todo: Todo) {
    event.preventDefault()
    await db.sql`
      DELETE FROM todos
        WHERE id = ${todo.id}
    `
  }
  if (results === undefined) {
    return <div className="loading">Loading -</div>
  }
  const todos = results.rows
  // The template below the heading is identical to the other patterns.
  // prettier-ignore
  return (
    <div id="optimistic-state" className="example">
      <h3>
        <span className="title">
          4. Through the DB
        </span>
      </h3>
      <ul>
        {todos.map((todo: Todo) => (
          <li key={todo.id}>
            <label>
              <input type="checkbox" checked={todo.completed}
                  onChange={() => updateTodo(todo)}
              />
              <span className={`title ${ todo.completed ? 'completed' : '' }`}>
                { todo.title }
              </span>
            </label>
            <a href="#delete" className="close"
                onClick={(event) => deleteTodo(event, todo)}>
              &#x2715;</a>
          </li>
        ))}
        {todos.length === 0 && (
          <li>All done </li>
        )}
      </ul>
      <form onSubmit={createTodo}>
        <input type="text" name="todo"
            placeholder="Type here -"
            required
        />
        <button type="submit">
          Add
        </button>
      </form>
    </div>
  )
}
```
```sql
-- This is the local database schema for PGlite.
-- It uses two tables: `todos_synced` and `todos_local`. These are combined
-- into a `todos` view that provides a merged view on both tables and supports
-- local live queries. Writes to the `todos` view are redirected using
-- `INSTEAD OF` triggers to the `todos_local` and `changes` tables.
-- The `todos_synced` table for immutable, synced state from the server.
CREATE TABLE IF NOT EXISTS todos_synced (
  id UUID PRIMARY KEY,
  title TEXT NOT NULL,
  completed BOOLEAN NOT NULL,
  created_at TIMESTAMP WITH TIME ZONE NOT NULL,
  -- Bookkeeping column.
  write_id UUID
);
-- The `todos_local` table for local optimistic state.
CREATE TABLE IF NOT EXISTS todos_local (
  id UUID PRIMARY KEY,
  title TEXT,
  completed BOOLEAN,
  created_at TIMESTAMP WITH TIME ZONE,
  -- Bookkeeping columns.
  changed_columns TEXT[],
  is_deleted BOOLEAN NOT NULL DEFAULT FALSE,
  write_id UUID NOT NULL
);
-- The `todos` view to combine the two tables on read.
CREATE OR REPLACE VIEW todos AS
  SELECT
    COALESCE(local.id, synced.id) AS id,
    CASE
      WHEN 'title' = ANY(local.changed_columns)
        THEN local.title
        ELSE synced.title
      END AS title,
    CASE
      WHEN 'completed' = ANY(local.changed_columns)
        THEN local.completed
        ELSE synced.completed
      END AS completed,
    CASE
      WHEN 'created_at' = ANY(local.changed_columns)
        THEN local.created_at
        ELSE synced.created_at
      END AS created_at
  FROM todos_synced AS synced
  FULL OUTER JOIN todos_local AS local
    ON synced.id = local.id
    WHERE local.id IS NULL OR local.is_deleted = FALSE;
-- Triggers to automatically remove local optimistic state when the corresponding
-- row syncs over the replication stream. Match on `write_id`, to allow local
-- state to be rebased on concurrent changes to the same row.
CREATE OR REPLACE FUNCTION delete_local_on_synced_insert_and_update_trigger()
RETURNS TRIGGER AS $$
BEGIN
  DELETE FROM todos_local
    WHERE id = NEW.id
      AND write_id IS NOT NULL
      AND write_id = NEW.write_id;
  RETURN NEW;
END;
$$ LANGUAGE plpgsql;
-- N.b.: deletes can be concurrent, but can't update the `write_id` and aren't
-- revertable (once a row is deleted, it would be re-created with an insert),
-- so its safe to just match on ID. You could implement revertable concurrent
-- deletes using soft deletes (which are actually updates).
CREATE OR REPLACE FUNCTION delete_local_on_synced_delete_trigger()
RETURNS TRIGGER AS $$
BEGIN
  DELETE FROM todos_local WHERE id = OLD.id;
  RETURN OLD;
END;
$$ LANGUAGE plpgsql;
CREATE OR REPLACE TRIGGER delete_local_on_synced_insert
AFTER INSERT OR UPDATE ON todos_synced
FOR EACH ROW
EXECUTE FUNCTION delete_local_on_synced_insert_and_update_trigger();
CREATE OR REPLACE TRIGGER delete_local_on_synced_delete
AFTER DELETE ON todos_synced
FOR EACH ROW
EXECUTE FUNCTION delete_local_on_synced_delete_trigger();
-- The local `changes` table for capturing and persisting a log
-- of local write operations that we want to sync to the server.
CREATE TABLE IF NOT EXISTS changes (
  id BIGSERIAL PRIMARY KEY,
  operation TEXT NOT NULL,
  value JSONB NOT NULL,
  write_id UUID NOT NULL,
  transaction_id XID8 NOT NULL
);
-- The following `INSTEAD OF` triggers:
-- 1. allow the app code to write directly to the view
-- 2. to capture write operations and write change messages into the
-- The insert trigger
CREATE OR REPLACE FUNCTION todos_insert_trigger()
RETURNS TRIGGER AS $$
DECLARE
  local_write_id UUID := gen_random_uuid();
BEGIN
  IF EXISTS (SELECT 1 FROM todos_synced WHERE id = NEW.id) THEN
    RAISE EXCEPTION 'Cannot insert: id already exists in the synced table';
  END IF;
  IF EXISTS (SELECT 1 FROM todos_local WHERE id = NEW.id) THEN
    RAISE EXCEPTION 'Cannot insert: id already exists in the local table';
  END IF;
  -- Insert into the local table.
  INSERT INTO todos_local (
    id,
    title,
    completed,
    created_at,
    changed_columns,
    write_id
  )
  VALUES (
    NEW.id,
    NEW.title,
    NEW.completed,
    NEW.created_at,
    ARRAY['title', 'completed', 'created_at'],
    local_write_id
  );
  -- Record the write operation in the change log.
  INSERT INTO changes (
    operation,
    value,
    write_id,
    transaction_id
  )
  VALUES (
    'insert',
    jsonb_build_object(
      'id', NEW.id,
      'title', NEW.title,
      'completed', NEW.completed,
      'created_at', NEW.created_at
    ),
    local_write_id,
    pg_current_xact_id()
  );
  RETURN NEW;
END;
$$ LANGUAGE plpgsql;
-- The update trigger
CREATE OR REPLACE FUNCTION todos_update_trigger()
RETURNS TRIGGER AS $$
DECLARE
  synced todos_synced%ROWTYPE;
  local todos_local%ROWTYPE;
  changed_cols TEXT[] := '{}';
  local_write_id UUID := gen_random_uuid();
BEGIN
  -- Fetch the corresponding rows from the synced and local tables
  SELECT * INTO synced FROM todos_synced WHERE id = NEW.id;
  SELECT * INTO local FROM todos_local WHERE id = NEW.id;
  -- If the row is not present in the local table, insert it
  IF NOT FOUND THEN
    -- Compare each column with the synced table and add to changed_cols if different
    IF NEW.title IS DISTINCT FROM synced.title THEN
      changed_cols := array_append(changed_cols, 'title');
    END IF;
    IF NEW.completed IS DISTINCT FROM synced.completed THEN
      changed_cols := array_append(changed_cols, 'completed');
    END IF;
    IF NEW.created_at IS DISTINCT FROM synced.created_at THEN
      changed_cols := array_append(changed_cols, 'created_at');
    END IF;
    INSERT INTO todos_local (
      id,
      title,
      completed,
      created_at,
      changed_columns,
      write_id
    )
    VALUES (
      NEW.id,
      NEW.title,
      NEW.completed,
      NEW.created_at,
      changed_cols,
      local_write_id
    );
  -- Otherwise, if the row is already in the local table, update it and adjust
  -- the changed_columns
  ELSE
    UPDATE todos_local
      SET
        title =
          CASE
            WHEN NEW.title IS DISTINCT FROM synced.title
              THEN NEW.title
              ELSE local.title
            END,
        completed =
          CASE
            WHEN NEW.completed IS DISTINCT FROM synced.completed
              THEN NEW.completed
              ELSE local.completed
            END,
        created_at =
          CASE
            WHEN NEW.created_at IS DISTINCT FROM synced.created_at
              THEN NEW.created_at
              ELSE local.created_at
            END,
        -- Set the changed_columns to columes that have both been marked as changed
        -- and have values that have actually changed.
        changed_columns = (
          SELECT array_agg(DISTINCT col) FROM (
            SELECT unnest(local.changed_columns) AS col
            UNION
            SELECT unnest(ARRAY['title', 'completed', 'created_at']) AS col
          ) AS cols
          WHERE (
            CASE
              WHEN col = 'title'
                THEN COALESCE(NEW.title, local.title) IS DISTINCT FROM synced.title
              WHEN col = 'completed'
                THEN COALESCE(NEW.completed, local.completed) IS DISTINCT FROM synced.completed
              WHEN col = 'created_at'
                THEN COALESCE(NEW.created_at, local.created_at) IS DISTINCT FROM synced.created_at
              END
          )
        ),
        write_id = local_write_id
      WHERE id = NEW.id;
  END IF;
  -- Record the update into the change log.
  INSERT INTO changes (
    operation,
    value,
    write_id,
    transaction_id
  )
  VALUES (
    'update',
    jsonb_strip_nulls(
      jsonb_build_object(
        'id', NEW.id,
        'title', NEW.title,
        'completed', NEW.completed,
        'created_at', NEW.created_at
      )
    ),
    local_write_id,
    pg_current_xact_id()
  );
  RETURN NEW;
END;
$$ LANGUAGE plpgsql;
-- The delete trigger
CREATE OR REPLACE FUNCTION todos_delete_trigger()
RETURNS TRIGGER AS $$
DECLARE
  local_write_id UUID := gen_random_uuid();
BEGIN
  -- Upsert a soft-deletion record in the local table.
  IF EXISTS (SELECT 1 FROM todos_local WHERE id = OLD.id) THEN
    UPDATE todos_local
    SET
      is_deleted = TRUE,
      write_id = local_write_id
    WHERE id = OLD.id;
  ELSE
    INSERT INTO todos_local (
      id,
      is_deleted,
      write_id
    )
    VALUES (
      OLD.id,
      TRUE,
      local_write_id
    );
  END IF;
  -- Record in the change log.
  INSERT INTO changes (
    operation,
    value,
    write_id,
    transaction_id
  )
  VALUES (
    'delete',
    jsonb_build_object(
      'id', OLD.id
    ),
    local_write_id,
    pg_current_xact_id()
  );
  RETURN OLD;
END;
$$ LANGUAGE plpgsql;
CREATE OR REPLACE TRIGGER todos_insert
INSTEAD OF INSERT ON todos
FOR EACH ROW
EXECUTE FUNCTION todos_insert_trigger();
CREATE OR REPLACE TRIGGER todos_update
INSTEAD OF UPDATE ON todos
FOR EACH ROW
EXECUTE FUNCTION todos_update_trigger();
CREATE OR REPLACE TRIGGER todos_delete
INSTEAD OF DELETE ON todos
FOR EACH ROW
EXECUTE FUNCTION todos_delete_trigger();
-- Notify on a `changes` topic whenever anything is added to the change log.
CREATE OR REPLACE FUNCTION changes_notify_trigger()
RETURNS TRIGGER AS $$
BEGIN
  NOTIFY changes;
  RETURN NEW;
END;
$$ LANGUAGE plpgsql;
CREATE OR REPLACE TRIGGER changes_notify
AFTER INSERT ON changes
FOR EACH ROW
EXECUTE FUNCTION changes_notify_trigger();
```
```typescript
import { type Operation } from '@electric-sql/client'
import { type PGliteWithLive } from '@electric-sql/pglite/live'
import api from '../../shared/app/client'
type Change = {
  id: number
  operation: Operation
  value: {
    id: string
    title?: string
    completed?: boolean
    created_at?: Date
  }
  write_id: string
  transaction_id: string
}
type SendResult = 'accepted' | 'rejected' | 'retry'
/*
 * Minimal, naive synchronization utility, just to illustrate the pattern of
 * `listen`ing to `changes` and `POST`ing them to the api server.
 */
export default class ChangeLogSynchronizer {
  #db: PGliteWithLive
  #position: number
  #hasChangedWhileProcessing: boolean = false
  #shouldContinue: boolean = true
  #status: 'idle' | 'processing' = 'idle'
  #abortController?: AbortController
  #unsubscribe?: () => Promise<void>
  constructor(db: PGliteWithLive, position = 0) {
    this.#db = db
    this.#position = position
  }
  /*
   * Start by listening for notifications.
   */
  async start(): Promise<void> {
    this.#abortController = new AbortController()
    this.#unsubscribe = await this.#db.listen('changes', this.handle.bind(this))
    this.process()
  }
  /*
   * On notify, either kick off processing or note down that there were changes
   * so we can process them straightaway on the next loop.
   */
  async handle(): Promise<void> {
    if (this.#status === 'processing') {
      this.#hasChangedWhileProcessing = true
      return
    }
    this.#status = 'processing'
    this.process()
  }
  // Process the changes by fetching them and posting them to the server.
  // If the changes are accepted then proceed, otherwise rollback or retry.
  async process(): Promise<void> {
    this.#hasChangedWhileProcessing = false
    const { changes, position } = await this.query()
    if (changes.length) {
      const result: SendResult = await this.send(changes)
      switch (result) {
        case 'accepted':
          await this.proceed(position)
          break
        case 'rejected':
          await this.rollback()
          break
        case 'retry':
          this.#hasChangedWhileProcessing = true
          break
      }
    }
    if (this.#hasChangedWhileProcessing && this.#shouldContinue) {
      return await this.process()
    }
    this.#status = 'idle'
  }
  /*
   * Fetch the current batch of changes
   */
  async query(): Promise<{ changes: Change[]; position: number }> {
    const { rows } = await this.#db.sql<Change>`
      SELECT * from changes
        WHERE id > ${this.#position}
        ORDER BY id asc
    `
    const position = rows.length ? rows.at(-1)!.id : this.#position
    return {
      changes: rows,
      position,
    }
  }
  /*
   * Send the current batch of changes to the server, grouped by transaction.
   */
  async send(changes: Change[]): Promise<SendResult> {
    const path = '/changes'
    const groups = Object.groupBy(changes, (x) => x.transaction_id)
    const sorted = Object.entries(groups).sort((a, b) =>
      a[0].localeCompare(b[0])
    )
    const transactions = sorted.map(([transaction_id, changes]) => {
      return {
        id: transaction_id,
        changes: changes,
      }
    })
    const signal = this.#abortController?.signal
    let response: Response | undefined
    try {
      response = await api.request(path, 'POST', transactions, signal)
    } catch (_err) {
      return 'retry'
    }
    if (response === undefined) {
      return 'retry'
    }
    if (response.ok) {
      return 'accepted'
    }
    return response.status < 500 ? 'rejected' : 'retry'
  }
  /*
   * Proceed by clearing the processed changes and moving the position forward.
   */
  async proceed(position: number): Promise<void> {
    await this.#db.sql`
      DELETE from changes
        WHERE id <= ${position}
    `
    this.#position = position
  }
  /*
   * Rollback with an extremely naive strategy: if any write is rejected, simply
   * wipe the entire local state.
   */
  async rollback(): Promise<void> {
    await this.#db.transaction(async (tx) => {
      await tx.sql`DELETE from changes`
      await tx.sql`DELETE from todos_local`
    })
  }
  /*
   * Stop synchronizing
   */
  async stop(): Promise<void> {
    this.#shouldContinue = false
    if (this.#abortController !== undefined) {
      this.#abortController.abort()
    }
    if (this.#unsubscribe !== undefined) {
      await this.#unsubscribe()
    }
  }
}
```
# Elixir client
Electric provides an [Elixir client](#how-to-use) that wraps the [HTTP API](/docs/api/http) into a higher-level stream interface and a [Phoenix integration](#phoenix-integration) that adds sync to your Phoenix application.
## How to use
The [`Electric.Client`](https://hex.pm/packages/electric_client) library allows you to stream [Shapes](/docs/guides/shapes) into your Elixir application. It's published to Hex as the [`electric_client`](https://hex.pm/packages/electric_client) package.
### Stream
The client exposes a [`stream/3`](https://hexdocs.pm/electric_client/Electric.Client.html#stream/3) that streams a [Shape Log](/docs/api/http#shape-log) into an [`Enumerable`](https://hexdocs.pm/elixir/Enumerable.html):
```elixir
Mix.install([:electric_client])
{:ok, client} = Electric.Client.new(base_url: "http://localhost:3000")
stream = Electric.Client.stream(client, "my_table", where: "something = true")
stream
|> Stream.each(&IO.inspect/1)
|> Stream.run()
```
You can materialise the shape stream into a variety of data structures. by matching on insert, update and delete operations and applying them to a Map or an Ecto struct. (See the [Redis example](/demos/redis) example and Typescript [Shape class](/docs/api/clients/typescript#shape) for reference).
### Ecto queries
The `stream/3` function also supports deriving the shape definition from an [`Ecto.Query`](https://hexdocs.pm/ecto/Ecto.Query.html):
```elixir
import Ecto.Query, only: [from: 2]
query = from(t in MyTable, where: t.something == true)
stream = Electric.Client.stream(client, query)
```
See the documentation at [hexdocs.pm/electric_client](https://hexdocs.pm/electric_client) for more details.
## Phoenix integration
Electric also provides an [`Electric.Phoenix`](https://hex.pm/packages/electric_phoenix) integration allows you to:
- sync data into a [front-end app](/docs/integrations/phoenix#front-end-sync) from a Postgres-backed Phoenix application; and
- add real-time streaming from Postgres into Phoenix LiveView via [Phoenix.Streams](/docs/integrations/phoenix#liveview-sync)
See the [Phoenix framework integration page](/docs/integrations/phoenix) for more details.
# Phoenix
[Phoenix](https://www.phoenixframework.org) is a full-stack web development framework for [Elixir](https://elixir-lang.org).
## Electric and Phoenix
Electric is [developed in Elixir](/product/electric#how-does-it-work) and provides [an Elixir client](/docs/api/clients/elixir) and deep Phoenix-framework integration in the form of the official [Phoenix.Sync](https://hexdocs.pm/phoenix_sync) library.
### Phoenix.Sync
Phoenix.Sync enables real-time sync for Postgres-backed [Phoenix](https://www.phoenixframework.org/) applications. You can use it to sync data into Elixir, `LiveView` and frontend web and mobile applications.
The library integrates with `Plug` and `Phoenix.{Controller, LiveView, Router, Stream}`. It uses [ElectricSQL](https://electric-sql.com) as the core sync engine, either as an embedded application dependency, or running as an external HTTP service.
The APIs map [Ecto queries](https://hexdocs.pm/ecto/Ecto.Query.html) to [Shapes](/docs/guides/shapes).
Documentation is available at [hexdocs.pm/phoenix_sync](https://hexdocs.pm/phoenix_sync).
## Usage
There are four key APIs:
- [`Phoenix.Sync.Client.stream/2`](https://hexdocs.pm/phoenix_sync/Phoenix.Sync.Client.html#stream/2) for low level usage in Elixir
- [`Phoenix.Sync.LiveView.sync_stream/4`](https://hexdocs.pm/phoenix_sync/Phoenix.Sync.LiveView.html#sync_stream/4) to sync into a LiveView stream
- [`Phoenix.Sync.Router.sync/2`](https://hexdocs.pm/phoenix_sync/Phoenix.Sync.Router.html#sync/2) macro to expose a statically defined shape in your Router
- [`Phoenix.Sync.Controller.sync_render/3`](https://hexdocs.pm/phoenix_sync/Phoenix.Sync.Controller.html#sync_render/3) to expose dynamically constructed shapes from a Controller
### Low level usage in Elixir
Use [`Phoenix.Sync.Client.stream/2`](https://hexdocs.pm/phoenix_sync/Phoenix.Sync.Client.html#stream/2) to convert an `Ecto.Query` into an Elixir `Stream`:
```elixir
stream = Phoenix.Sync.Client.stream(Todos.Todo)
stream =
  Ecto.Query.from(t in Todos.Todo, where: t.completed == false)
  |> Phoenix.Sync.Client.stream()
```
### Sync into a LiveView stream
Swap out `Phoenix.LiveView.stream/3` for [`Phoenix.Sync.LiveView.sync_stream/4`](https://hexdocs.pm/phoenix_sync/Phoenix.Sync.LiveView.html#sync_stream/4) to automatically keep a LiveView up-to-date with the state of your Postgres database:
```elixir
defmodule MyWeb.MyLive do
  use Phoenix.LiveView
  import Phoenix.Sync.LiveView
  def mount(_params, _session, socket) do
    {:ok, sync_stream(socket, :todos, Todos.Todo)}
  end
  def handle_info({:sync, event}, socket) do
    {:noreply, sync_stream_update(socket, event)}
  end
end
```
LiveView takes care of automatically keeping the front-end up-to-date with the assigned stream. What Phoenix.Sync does is automatically keep the _stream_ up-to-date with the state of the database.
This means you can build fully end-to-end real-time multi-user applications without writing Javascript _and_ without worrying about message delivery, reconnections, cache invalidation or polling the database for changes.
### Sync shapes through your Router
Use the [`Phoenix.Sync.Router.sync/2`](https://hexdocs.pm/phoenix_sync/Phoenix.Sync.Router.html#sync/2) macro to expose statically (compile-time) defined shapes in your Router:
```elixir
defmodule MyWeb.Router do
  use Phoenix.Router
  import Phoenix.Sync.Router
  pipeline :sync do
    plug :my_auth
  end
  scope "/shapes" do
    pipe_through :sync
    sync "/todos", Todos.Todo
  end
end
```
Because the shapes are exposed through your Router, the client connects through your existing Plug middleware.
### Sync dynamic shapes from a Controller
Sync shapes from any standard Controller using the [`Phoenix.Sync.Controller.sync_render/3`](https://hexdocs.pm/phoenix_sync/Phoenix.Sync.Controller.html#sync_render/3) view function:
```elixir
defmodule Phoenix.Sync.LiveViewTest.TodoController do
  use Phoenix.Controller
  import Phoenix.Sync.Controller
  import Ecto.Query, only: [from: 2]
  def show(conn, %{"done" => done} = params) do
    sync_render(conn, params, from(t in Todos.Todo, where: t.done == ^done))
  end
  def show_mine(%{assigns: %{current_user: user_id}} = conn, params) do
    sync_render(conn, params, from(t in Todos.Todo, where: t.owner_id == ^user_id))
  end
end
```
### Consume shapes in the frontend
You can sync _into_ any client in any language that [speaks HTTP and JSON](/docs/api/http). using the Electric [Typescript client](/docs/api/clients/typescript):
```typescript
import { Shape, ShapeStream } from "@electric-sql/client";
const stream = new ShapeStream({
  url: `/shapes/todos`,
});
const shape = new Shape(stream);
// The callback runs every time the data changes.
shape.subscribe((data) => console.log(data));
```
Or binding a shape to a component using the [React bindings](/docs/integrations/react):
```tsx
import { useShape } from "@electric-sql/react";
const MyComponent = () => {
  const { data } = useShape({
    url: `shapes/todos`,
  });
  return <List todos={data} />;
};
```
See the Electric [demos](/demos) and [documentation](/docs/intro) for more client-side usage examples.
## Installation and configuration
`Phoenix.Sync` can be used in two modes:
1. `:embedded` where Electric is included as an application dependency and Phoenix.Sync consumes data internally using Elixir APIs
2. `:http` where Electric does _not_ need to be included as an application dependency and Phoenix.Sync consumes data from an external Electric service using it's [HTTP API](/docs/api/http)
### Embedded mode
In `:embedded` mode, Electric must be included an application dependency but does not expose an HTTP API (internally or externally). Messages are streamed internally between Electric and Phoenix.Sync using Elixir function APIs. The only HTTP API for sync is that exposed via your Phoenix Router using the `sync/2` macro and `sync_render/3` function.
Example config:
```elixir
# mix.exs
defp deps do
  [
    {:electric, ">= 1.0.0-beta.20"},
    {:phoenix_sync, "~> 0.3"}
  ]
end
# config/config.exs
config :phoenix_sync,
  env: config_env(),
  mode: :embedded,
  repo: MyApp.Repo
# application.ex
children = [
  MyApp.Repo,
  # ...
  {MyApp.Endpoint, phoenix_sync: Phoenix.Sync.plug_opts()}
]
```
### HTTP
In `:http` mode, Electric does not need to be included as an application dependency. Instead, Phoenix.Sync consumes data from an external Electric service over HTTP.
```elixir
# mix.exs
defp deps do
  [
    {:phoenix_sync, "~> 0.3"}
  ]
end
# config/config.exs
config :phoenix_sync,
  env: config_env(),
  mode: :http,
  url: "https://api.electric-sql.cloud",
  credentials: [
    secret: "...",    # required
    source_id: "..."  # optional, required for Electric Cloud
  ]
# application.ex
children = [
  MyApp.Repo,
  # ...
  {MyApp.Endpoint, phoenix_sync: Phoenix.Sync.plug_opts()}
]
```
# Security
How to secure data access and [encrypt data](#encryption) with Electric.
## Data access
Electric is a [sync service](/product/electric) that runs in front of Postgres. It connects to a Postgres database using a [`DATABASE_URL`](/docs/api/config#database-url) and exposes the data in that database via an [HTTP API](/docs/api/http).
This API is [public by default](#public-by-default). It should be secured in production using an [API token](#api-token), [network security](#network-security) and/or an [authorization proxy](#authorization).
### Public by default
Electric connects to Postgres as a normal [database user](https://www.postgresql.org/docs/current/user-manag.html). It then exposes access to **any data** that its database user can access in Postgres to **any client** that can connect to the Electric HTTP API.
You generally do _not_ want to expose public access to the contents of your database, so you **must** secure access to the Electric HTTP API.
### Network security
One way of securing access to Electric is to use a network firewall or IP whitelist.
You can often configure this using the networking rules of your cloud provider. Or you can use these to restrict public access to Electric and only expose Electric via a reverse-proxy such as Nginx or Caddy. This reverse proxy can then enforce network security rules, using Caddy's [`remote-ip` request matcher](https://caddyserver.com/docs/caddyfile/matchers#remote-ip):
```hcl
@denied not remote_ip 100.200.30.40 100.200.30.41
abort @denied
```
This approach is useful when you're using Electric to sync into trusted infrastructure. However, it doesn't help when you're syncing data into client devices, like apps and web browsers. For those, you need to restrict access using an authorizing proxy.
### Authorization
Electric is designed to run behind an [authorizing proxy](/docs/guides/auth#requests-can-be-proxied).
This is the primary method for securing data access to clients and apps and is documented in detail, with examples, in the [Auth guide](/docs/guides/auth).
### API token
Access to Electric can be secured with an [API token](/docs/api/config#electric-secret). This is a secret string that can be set when starting Electric and will be used to authenticate requests to the Electric HTTP API. When an API token is set, Electric will require all requests to include the API token.
The token should *not* be sent from the client as it will be exposed in the HTTP requests. Instead, it should be added by the [authorizing proxy](/docs/guides/auth#requests-can-be-proxied) when proxying requests to Electric.
### Syncing into PGlite
PGlite.dev is an embedded Postgres database you can run in the browser. You can use Electric to sync between a cloud Postgres and an embedded PGlite instance.
```tsx
import { PGlite } from '@electric-sql/pglite'
import { live } from '@electric-sql/pglite/live'
import { electricSync } from '@electric-sql/pglite-sync'
import { useLiveQuery } from '@electric-sql/pglite-react'
// Create a persistent local PGlite database
const pg = await PGlite.create({
  dataDir: 'idb://my-database',
  extensions: {
    electric: electricSync(),
    live
  }
})
// Setup the local database schema
await pg.exec(`
  CREATE TABLE IF NOT EXISTS items (
    id SERIAL PRIMARY KEY,
  );
`)
// Establish a persistent shape subscription
await pg.electric.syncShapeToTable({
  shape: { url: `${BASE_URL}/v1/shape` },
  table: 'items',
  primaryKey: ['id'],
})
// Bind data to your components using live queries
// against the local embedded database
const Component = () => {
  const items = useLiveQuery(
    `SELECT * FROM items;`
  )
  return <pre>{JSON.stringify(items)}</pre>
}
```